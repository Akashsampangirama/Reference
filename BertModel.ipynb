{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BertModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czJmDqZrzoYw",
        "outputId": "420d4a6a-5a32-43cb-f61e-0141072dfb59"
      },
      "source": [
        "!pip install transformers==3.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.1.94)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZH1m14azZaW"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "import transformers\r\n",
        "from transformers import AutoModel, BertTokenizerFast\r\n",
        "\r\n",
        "# specify GPU\r\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "1r4x2JYdzwej",
        "outputId": "4adfc514-36c0-4470-e22a-576df480db18"
      },
      "source": [
        "from google.colab import files\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9dc84aab-60be-4ee1-bfcc-102a117ef68a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9dc84aab-60be-4ee1-bfcc-102a117ef68a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Training_sample_23DEC2020.csv to Training_sample_23DEC2020.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGEhKf9D5U20"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Z7L-p5cr0Pa_",
        "outputId": "c6bcea64-324e-442d-ce2e-1fda4d917a98"
      },
      "source": [
        "import io\r\n",
        "#df = pd.read_csv(io.BytesIO(uploaded['Training_sample_23DEC2020.csv']))\r\n",
        "df = pd.read_csv('Training_sample_23DEC2020.csv')\r\n",
        "df.head(5)\r\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>CPG_CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2pc Stoneware Salad Tongs - Hearth &amp; Hand™ wit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NAUTICA SLEEP SET</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Header AB NEW 3' Save &amp; Celebrate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AriZona Green Tea with Ginseng and Honey - 20 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Girls' Shopkins® Boy Shorts 2pk - Pink S</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  CPG_CATEGORY\n",
              "0  2pc Stoneware Salad Tongs - Hearth & Hand™ wit...             0\n",
              "1                                  NAUTICA SLEEP SET             0\n",
              "2                  Header AB NEW 3' Save & Celebrate             0\n",
              "3  AriZona Green Tea with Ginseng and Honey - 20 ...             1\n",
              "4           Girls' Shopkins® Boy Shorts 2pk - Pink S             0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JffMtcnKcVVx"
      },
      "source": [
        "df = df.sample(n=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qWyFfNo1AYa"
      },
      "source": [
        "# split train dataset into train, validation and test sets\r\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['title'], df['CPG_CATEGORY'], \r\n",
        "                                                                    random_state=2018, \r\n",
        "                                                                    test_size=0.3, \r\n",
        "                                                                    stratify=df['CPG_CATEGORY'])\r\n",
        "\r\n",
        "\r\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \r\n",
        "                                                                random_state=2018, \r\n",
        "                                                                test_size=0.5, \r\n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tki34RBh1Fzp"
      },
      "source": [
        "# import BERT-base pretrained model\r\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\r\n",
        "\r\n",
        "# Load the BERT tokenizer\r\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yCsHjAA1vND",
        "outputId": "6b950eb2-112c-4097-9e3c-36944c7876e0"
      },
      "source": [
        "# sample data\r\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\r\n",
        "\r\n",
        "# encode text\r\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True)\r\n",
        "\r\n",
        "# output\r\n",
        "print(sent_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Ux6Ifim12Kxa",
        "outputId": "880d4939-7b3b-40b6-aeb5-ed8490d3d4ea"
      },
      "source": [
        "seq_len = [len(i.split()) for i in train_text]\r\n",
        "\r\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fad2d5f7588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVrElEQVR4nO3df5BdZX3H8fe3QSC6NgFxdpgk02DN2MGktWQLdGydXWghgGPoDFoso4kTm5kWK23TKaEdC1WYxraU6qh0UpMaxLpi1CHlRzEN2XGcaQAjyAaiZYXYZocmrQmxq6m69Ns/7rPldrn7657de2+S92tmZ895znPO/d7nJvez5zln70ZmIkk6tf1EuwuQJLWfYSBJMgwkSYaBJAnDQJIEnNbuApp1zjnn5NKlSxtu+/73v8+rXvWq1hY0A9ZXjfVVY33VnOj17d279z8z87Uv25CZJ+TXypUrcyK7d++ecFsnsL5qrK8a66vmRK8P+Fo2eE91mkiSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSZzAH0dxIlq68X4ANqwYZW1ZnsiBTVe1oiRJAjwzkCRhGEiSMAwkSRgGkiS8gKxxlm683wvc0inIMwNJkmEgSTIMJEkYBpIkDANJEoaBJIlphEFEbI2IwxGxr67tLyLimxHxZER8KSIW1m27KSKGIuJbEXF5Xfuq0jYUERvr2s+LiEdK++ci4vTZfIKSpKlN58zgU8CqcW07geWZ+bPAvwA3AUTE+cC1wBvLPp+IiHkRMQ/4OHAFcD7wztIX4MPAHZn5euAosK7SM5IkzdiUYZCZXwGOjGv7cmaOltU9wOKyvBroz8wfZuZzwBBwYfkaysxnM/NHQD+wOiICuATYXvbfBlxd8TlJkmYoMnPqThFLgfsyc3mDbf8AfC4z746IjwF7MvPusm0L8GDpuioz31va3wVcBNxS+r++tC8BHmz0OGX7emA9QHd398r+/v6G9Y6MjNDV1TXl82q1weFjAHTPh0PHJ++7YtGCFlT0coPDxzq6Pujc13eM9VVjfdVMVV9fX9/ezOwZ317p4ygi4o+BUeAzVY4zXZm5GdgM0NPTk729vQ37DQwMMNG2dlpb9/cMbh+cfOgPXNfbgopebm35OIpOrQ869/UdY33VWF81zdbXdBhExFrgrcCl+dLpxTCwpK7b4tLGBO3fBRZGxGll2qm+vySpRZq6tTQiVgF/CLwtM39Qt2kHcG1EnBER5wHLgEeBx4Bl5c6h06ldZN5RQmQ3cE3Zfw1wb3NPRZLUrOncWvpZ4J+BN0TEwYhYB3wMeDWwMyKeiIi/AcjMp4B7gKeBfwSuz8wXy0/97wMeAvYD95S+ADcCvx8RQ8BrgC2z+gwlSVOacpooM9/ZoHnCN+zMvA24rUH7A8ADDdqfpXa3kSSpTfwNZEmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSxDTCICK2RsThiNhX13Z2ROyMiGfK97NKe0TERyNiKCKejIgL6vZZU/o/ExFr6tpXRsRg2eejERGz/SQlSZObzpnBp4BV49o2Arsycxmwq6wDXAEsK1/rgTuhFh7AzcBFwIXAzWMBUvr8Zt1+4x9LkjTHpgyDzPwKcGRc82pgW1neBlxd135X1uwBFkbEucDlwM7MPJKZR4GdwKqy7Sczc09mJnBX3bEkSS0StffgKTpFLAXuy8zlZf2FzFxYlgM4mpkLI+I+YFNmfrVs2wXcCPQCZ2bmraX9A8BxYKD0/5XS/svAjZn51gnqWE/tjIPu7u6V/f39DesdGRmhq6trGk+/tQaHjwHQPR8OHZ+874pFC1pQ0csNDh/r6Pqgc1/fMdZXjfVVM1V9fX19ezOzZ3z7aVUfODMzIqZOlFmQmZuBzQA9PT3Z29vbsN/AwAATbWuntRvvB2DDilFuH5x86A9c19uCil5u7cb7O7o+6NzXd4z1VWN91TRbX7N3Ex0qUzyU74dL+zCwpK7f4tI2WfviBu2SpBZqNgx2AGN3BK0B7q1rf3e5q+hi4FhmPg88BFwWEWeVC8eXAQ+Vbd+LiIvLdNO7644lSWqRKaeJIuKz1Ob8z4mIg9TuCtoE3BMR64DvAO8o3R8ArgSGgB8A7wHIzCMR8SHgsdLvg5k5dlH6t6ndsTQfeLB8SZJaaMowyMx3TrDp0gZ9E7h+guNsBbY2aP8asHyqOiRJc8ffQJYkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkScBpVXaOiN8D3gskMAi8BzgX6AdeA+wF3pWZP4qIM4C7gJXAd4Ffz8wD5Tg3AeuAF4H3Z+ZDVeo6lSzdeP+0+h3YdNUcVyLpRNb0mUFELALeD/Rk5nJgHnAt8GHgjsx8PXCU2ps85fvR0n5H6UdEnF/2eyOwCvhERMxrti5J0sxVOjMo+8+PiB8DrwSeBy4BfqNs3wbcAtwJrC7LANuBj0VElPb+zPwh8FxEDAEXAv9csTbVme4ZhKRTU2Rm8ztH3ADcBhwHvgzcAOwpP/0TEUuABzNzeUTsA1Zl5sGy7dvARdQCYk9m3l3at5R9tjd4vPXAeoDu7u6V/f39DesaGRmhq6ur6ec1VwaHjwHQPR8OHZ+874pFC2Z0zNk0m/XNhU59fcdYXzXWV81U9fX19e3NzJ7x7U2fGUTEWdR+qj8PeAH4PLVpnjmTmZuBzQA9PT3Z29vbsN/AwAATbWunteWn8w0rRrl9cPKhP3Bd74yOOZtms7650Kmv7xjrq8b6qmm2vip3E/0K8Fxm/kdm/hj4IvBmYGFEjL2TLAaGy/IwsASgbF9A7ULy/7U32EeS1AJVwuBfgYsj4pVl7v9S4GlgN3BN6bMGuLcs7yjrlO0PZ22OagdwbUScERHnAcuARyvUJUmaoaaniTLzkYjYDnwdGAUepzaFcz/QHxG3lrYtZZctwKfLBeIj1O4gIjOfioh7qAXJKHB9Zr7YbF2SpJmrdDdRZt4M3Dyu+VlqdwON7/vfwNsnOM5t1C5ES5LawN9AliQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEmi+p+91Bzxz1RKaiXPDCRJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSaJiGETEwojYHhHfjIj9EfGLEXF2ROyMiGfK97NK34iIj0bEUEQ8GREX1B1nTen/TESsqfqkJEkzU/XM4CPAP2bmzwA/B+wHNgK7MnMZsKusA1wBLCtf64E7ASLibOBm4CLgQuDmsQCRJLVG02EQEQuAtwBbADLzR5n5ArAa2Fa6bQOuLsurgbuyZg+wMCLOBS4Hdmbmkcw8CuwEVjVblyRp5iIzm9sx4k3AZuBpamcFe4EbgOHMXFj6BHA0MxdGxH3Apsz8atm2C7gR6AXOzMxbS/sHgOOZ+ZcNHnM9tbMKuru7V/b39zesbWRkhK6urqae11waHD4GQPd8OHS8zcVMYjr1rVi0oDXFNNCpr+8Y66vG+qqZqr6+vr69mdkzvr3KR1ifBlwA/E5mPhIRH+GlKSEAMjMjorm0aSAzN1MLIHp6erK3t7dhv4GBASba1k5ry8dSb1gxyu2Dnfvp4dOp78B1va0ppoFOfX3HWF811ldNs/VVuWZwEDiYmY+U9e3UwuFQmf6hfD9ctg8DS+r2X1zaJmqXJLVI02GQmf8O/FtEvKE0XUptymgHMHZH0Brg3rK8A3h3uavoYuBYZj4PPARcFhFnlQvHl5U2SVKLVJ2r+B3gMxFxOvAs8B5qAXNPRKwDvgO8o/R9ALgSGAJ+UPqSmUci4kPAY6XfBzPzSMW6Wsq/SibpRFcpDDLzCeBlFyKonSWM75vA9RMcZyuwtUotkqTm+RvIkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJWQiDiJgXEY9HxH1l/byIeCQihiLicxFxemk/o6wPle1L645xU2n/VkRcXrUmSdLMzMaZwQ3A/rr1DwN3ZObrgaPAutK+Djha2u8o/YiI84FrgTcCq4BPRMS8WahLkjRNlcIgIhYDVwGfLOsBXAJsL122AVeX5dVlnbL90tJ/NdCfmT/MzOeAIeDCKnVJkmYmMrP5nSO2A38GvBr4A2AtsKf89E9ELAEezMzlEbEPWJWZB8u2bwMXAbeUfe4u7VvKPtvHPRwRsR5YD9Dd3b2yv7+/YV0jIyN0dXU1/bxmanD42Iz6d8+HQ8fnqJhZMJ36Vixa0JpiGmj16ztT1leN9VUzVX19fX17M7NnfPtpzT5gRLwVOJyZeyOit9njzERmbgY2A/T09GRvb+OHHRgYYKJtc2Htxvtn1H/DilFuH2x66OfcdOo7cF1va4ppoNWv70xZXzXWV02z9VV5R3oz8LaIuBI4E/hJ4CPAwog4LTNHgcXAcOk/DCwBDkbEacAC4Lt17WPq91GHWjrNADyw6ao5rkTSbGj6mkFm3pSZizNzKbULwA9n5nXAbuCa0m0NcG9Z3lHWKdsfztoc1Q7g2nK30XnAMuDRZuuSJM3cXMxV3Aj0R8StwOPAltK+Bfh0RAwBR6gFCJn5VETcAzwNjALXZ+aLc1CXJGkCsxIGmTkADJTlZ2lwN1Bm/jfw9gn2vw24bTZqkSTNnL+BLEmak2mik8Z0L5JK0onOMwNJkmEgSTIMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCThXzrTHJvuX4s7sOmqOa5E0mQ8M5AkGQaSJMNAkkSFMIiIJRGxOyKejoinIuKG0n52ROyMiGfK97NKe0TERyNiKCKejIgL6o61pvR/JiLWVH9akqSZqHJmMApsyMzzgYuB6yPifGAjsCszlwG7yjrAFcCy8rUeuBNq4QHcDFwEXAjcPBYgkqTWaDoMMvP5zPx6Wf4vYD+wCFgNbCvdtgFXl+XVwF1ZswdYGBHnApcDOzPzSGYeBXYCq5qtS5I0c5GZ1Q8SsRT4CrAc+NfMXFjaAziamQsj4j5gU2Z+tWzbBdwI9AJnZuatpf0DwPHM/MsGj7Oe2lkF3d3dK/v7+xvWMzIyQldXV+XnNTh8rPIxGumeD4eOz8mhZ0U76luxaMG0+87W6ztXrK8a66tmqvr6+vr2ZmbP+PbKv2cQEV3AF4Dfzczv1d7/azIzI6J62rx0vM3AZoCenp7s7e1t2G9gYICJts3E2mneIz9TG1aMcvtg5/6KRzvqO3Bd77T7ztbrO1esrxrrq6bZ+irdTRQRr6AWBJ/JzC+W5kNl+ofy/XBpHwaW1O2+uLRN1C5JapEqdxMFsAXYn5l/VbdpBzB2R9Aa4N669neXu4ouBo5l5vPAQ8BlEXFWuXB8WWmTJLVIlbmANwPvAgYj4onS9kfAJuCeiFgHfAd4R9n2AHAlMAT8AHgPQGYeiYgPAY+Vfh/MzCMV6tJJbnD42LSm8PyIC2n6mg6DciE4Jth8aYP+CVw/wbG2AlubrUWSVE3nXsXUKWW6H2gHsGHFHBYinaL8OApJkmEgSTIMJEmcotcMZjI/LUmnAs8MJEmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKn6N8z0Klhun+34sCmq+a4EqnzeWYgSTIMJEkdNE0UEauAjwDzgE9m5qY2l6RThNNJUoeEQUTMAz4O/CpwEHgsInZk5tPtrUx6yXRDY8OKUdZOo6/hok7SEWEAXAgMZeazABHRD6wGDAOdtKYbLrNtw4pRetvyyOpkkZntroGIuAZYlZnvLevvAi7KzPeN67ceWF9W3wB8a4JDngP85xyVOxusrxrrq8b6qjnR6/upzHzt+MZOOTOYlszcDGyeql9EfC0ze1pQUlOsrxrrq8b6qjlZ6+uUu4mGgSV164tLmySpBTolDB4DlkXEeRFxOnAtsKPNNUnSKaMjpokyczQi3gc8RO3W0q2Z+VSFQ045ldRm1leN9VVjfdWclPV1xAVkSVJ7dco0kSSpjQwDSdLJFQYRsSoivhURQxGxsd31NBIRByJiMCKeiIivdUA9WyPicETsq2s7OyJ2RsQz5ftZHVbfLRExXMbwiYi4so31LYmI3RHxdEQ8FRE3lPaOGMNJ6uuIMYyIMyPi0Yj4RqnvT0v7eRHxSPm//LlyY0kn1fepiHiubvze1I76Si3zIuLxiLivrDc3dpl5UnxRu/D8beB1wOnAN4Dz211XgzoPAOe0u466et4CXADsq2v7c2BjWd4IfLjD6rsF+IN2j12p5VzggrL8auBfgPM7ZQwnqa8jxhAIoKssvwJ4BLgYuAe4trT/DfBbHVbfp4Br2j1+pa7fB/4euK+sNzV2J9OZwf99pEVm/ggY+0gLTSIzvwIcGde8GthWlrcBV7e0qDoT1NcxMvP5zPx6Wf4vYD+wiA4Zw0nq6whZM1JWX1G+ErgE2F7a2zl+E9XXESJiMXAV8MmyHjQ5didTGCwC/q1u/SAd9I++TgJfjoi95eM1OlF3Zj5flv8d6G5nMRN4X0Q8WaaR2jaNVS8ilgI/T+2nx44bw3H1QYeMYZnmeAI4DOykdob/QmaOli5t/b88vr7MHBu/28r43RERZ7SpvL8G/hD4n7L+Gpocu5MpDE4Uv5SZFwBXANdHxFvaXdBksnau2TE/CRV3Aj8NvAl4Hri9veVARHQBXwB+NzO/V7+tE8awQX0dM4aZ+WJmvonaJw9cCPxMu2ppZHx9EbEcuIlanb8AnA3c2Oq6IuKtwOHM3DsbxzuZwuCE+EiLzBwu3w8DX6L2j7/THIqIcwHK98Ntruf/ycxD5T/o/wB/S5vHMCJeQe2N9jOZ+cXS3DFj2Ki+ThvDUtMLwG7gF4GFETH2S7Ed8X+5rr5VZfotM/OHwN/RnvF7M/C2iDhAbVr8Emp/E6apsTuZwqDjP9IiIl4VEa8eWwYuA/ZNvldb7ADWlOU1wL1trOVlxt5ki1+jjWNY5mi3APsz86/qNnXEGE5UX6eMYUS8NiIWluX51P6myX5qb7rXlG7tHL9G9X2zLuiD2px8y8cvM2/KzMWZuZTa+93DmXkdzY5du6+Ez/JV9Sup3S3xbeCP211Pg/peR+0up28AT3VCjcBnqU0T/Jja/OI6avOOu4BngH8Czu6w+j4NDAJPUnvTPbeN9f0StSmgJ4EnyteVnTKGk9TXEWMI/CzweKljH/Anpf11wKPAEPB54IwOq+/hMn77gLspdxy18d9hLy/dTdTU2PlxFJKkk2qaSJLUJMNAkmQYSJIMA0kShoEkCcNAkoRhIEkC/hcw3PhVL4bM/AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DES3rIix2PSr"
      },
      "source": [
        "# tokenize and encode sequences in the training set\r\n",
        "tokens_train = tokenizer.batch_encode_plus(\r\n",
        "    train_text.tolist(),\r\n",
        "    max_length = 15,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True\r\n",
        ")\r\n",
        "\r\n",
        "# tokenize and encode sequences in the validation set\r\n",
        "tokens_val = tokenizer.batch_encode_plus(\r\n",
        "    val_text.tolist(),\r\n",
        "    max_length = 15,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True\r\n",
        ")\r\n",
        "\r\n",
        "# tokenize and encode sequences in the test set\r\n",
        "tokens_test = tokenizer.batch_encode_plus(\r\n",
        "    test_text.tolist(),\r\n",
        "    max_length = 15,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSSfZ_T_2WO_"
      },
      "source": [
        "## convert lists to tensors\r\n",
        "\r\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\r\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\r\n",
        "train_y = torch.tensor(train_labels.tolist())\r\n",
        "\r\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\r\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\r\n",
        "val_y = torch.tensor(val_labels.tolist())\r\n",
        "\r\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\r\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\r\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooKzbRA-MWIj",
        "outputId": "92535d34-f7af-4e7b-fa50-60d4ed91a45c"
      },
      "source": [
        "from tensorflow.python.client import device_lib\r\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 12062829588593279867, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 13812936256\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 10846970574736532961\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtMZJ8ot2bGC"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "#define a batch size\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\r\n",
        "\r\n",
        "# sampler for sampling the data during training\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "\r\n",
        "# dataLoader for train set\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\r\n",
        "\r\n",
        "# sampler for sampling the data during training\r\n",
        "val_sampler = SequentialSampler(val_data)\r\n",
        "\r\n",
        "# dataLoader for validation set\r\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhzVPsPO2en3",
        "outputId": "80df8716-06b2-417e-c305-b980dd33c017"
      },
      "source": [
        "test_seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  4993,  6804,  ...,     0,     0,     0],\n",
              "        [  101,  1996,  3800,  ...,  6174,  6031,   102],\n",
              "        [  101, 13007,  2422,  ...,  1013,  2385,   102],\n",
              "        ...,\n",
              "        [  101,  8738,  1021,  ...,  2243,  1013,   102],\n",
              "        [  101,  2191,  1011,  ...,  4897,  1027,   102],\n",
              "        [  101,  6927,  3917,  ...,  2630,  2260,   102]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jPdOzbp2jdM"
      },
      "source": [
        "# freeze all the parameters\r\n",
        "for param in bert.parameters():\r\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL-g8jx32msu"
      },
      "source": [
        "class BERT_Arch(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, bert):\r\n",
        "      \r\n",
        "      super(BERT_Arch, self).__init__()\r\n",
        "\r\n",
        "      self.bert = bert \r\n",
        "      \r\n",
        "      # dropout layer\r\n",
        "      self.dropout = nn.Dropout(0.1)\r\n",
        "      \r\n",
        "      # relu activation function\r\n",
        "      self.relu =  nn.ReLU()\r\n",
        "\r\n",
        "      # dense layer 1\r\n",
        "      self.fc1 = nn.Linear(768,512)\r\n",
        "      \r\n",
        "      # dense layer 2 (Output layer)\r\n",
        "      self.fc2 = nn.Linear(512,2)\r\n",
        "\r\n",
        "      #softmax activation function\r\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    #define the forward pass\r\n",
        "    def forward(self, sent_id, mask):\r\n",
        "\r\n",
        "      #pass the inputs to the model  \r\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\r\n",
        "      \r\n",
        "      x = self.fc1(cls_hs)\r\n",
        "\r\n",
        "      x = self.relu(x)\r\n",
        "\r\n",
        "      x = self.dropout(x)\r\n",
        "\r\n",
        "      # output layer\r\n",
        "      x = self.fc2(x)\r\n",
        "      \r\n",
        "      # apply softmax activation\r\n",
        "      x = self.softmax(x)\r\n",
        "\r\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfvp06eb2qMu"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\r\n",
        "model = BERT_Arch(bert)\r\n",
        "\r\n",
        "# push the model to GPU\r\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YlTwa9F2tvi"
      },
      "source": [
        "# optimizer from hugging face transformers\r\n",
        "from transformers import AdamW\r\n",
        "\r\n",
        "# define the optimizer\r\n",
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  lr = 1e-5)          # learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhNGAF1V210n",
        "outputId": "01fc0c1b-e63d-43df-dc73-4023fd76aab8"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\r\n",
        "\r\n",
        "#compute the class weights\r\n",
        "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\r\n",
        "\r\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Weights: [0.74553743 1.51817472]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WotE1s3l29Ud"
      },
      "source": [
        "# converting list of class weights to a tensor\r\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\r\n",
        "\r\n",
        "# push to GPU\r\n",
        "weights = weights.to(device)\r\n",
        "\r\n",
        "# define the loss function\r\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \r\n",
        "\r\n",
        "# number of training epochs\r\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opTovPlS3NQZ"
      },
      "source": [
        "# function to train the model\r\n",
        "def train():\r\n",
        "  \r\n",
        "  model.train()\r\n",
        "\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  \r\n",
        "  # empty list to save model predictions\r\n",
        "  total_preds=[]\r\n",
        "  \r\n",
        "  # iterate over batches\r\n",
        "  for step,batch in enumerate(train_dataloader):\r\n",
        "    \r\n",
        "    # progress update after every 50 batches.\r\n",
        "    if step % 500 == 0 and not step == 0:\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n",
        "\r\n",
        "    # push the batch to gpu\r\n",
        "    batch = [r.to(device) for r in batch]\r\n",
        " \r\n",
        "    sent_id, mask, labels = batch\r\n",
        "\r\n",
        "    # clear previously calculated gradients \r\n",
        "    model.zero_grad()        \r\n",
        "\r\n",
        "    # get model predictions for the current batch\r\n",
        "    preds = model(sent_id, mask)\r\n",
        "\r\n",
        "    # compute the loss between actual and predicted values\r\n",
        "    loss = cross_entropy(preds, labels)\r\n",
        "\r\n",
        "    # add on to the total loss\r\n",
        "    total_loss = total_loss + loss.item()\r\n",
        "\r\n",
        "    # backward pass to calculate the gradients\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "    # update parameters\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # model predictions are stored on GPU. So, push it to CPU\r\n",
        "    preds=preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "    # append the model predictions\r\n",
        "    total_preds.append(preds)\r\n",
        "\r\n",
        "  # compute the training loss of the epoch\r\n",
        "  avg_loss = total_loss / len(train_dataloader)\r\n",
        "  \r\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "  #returns the loss and predictions\r\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8PM5s7p3jto"
      },
      "source": [
        "# function for evaluating the model\r\n",
        "def evaluate():\r\n",
        "  \r\n",
        "  print(\"\\nEvaluating...\")\r\n",
        "  \r\n",
        "  # deactivate dropout layers\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  \r\n",
        "  # empty list to save the model predictions\r\n",
        "  total_preds = []\r\n",
        "\r\n",
        "  # iterate over batches\r\n",
        "  for step,batch in enumerate(val_dataloader):\r\n",
        "    \r\n",
        "    # Progress update every 50 batches.\r\n",
        "    if step % 50 == 0 and not step == 0:\r\n",
        "      \r\n",
        "      # Calculate elapsed time in minutes.\r\n",
        "      #elapsed = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "      # Report progress.\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n",
        "\r\n",
        "    # push the batch to gpu\r\n",
        "    batch = [t.to(device) for t in batch]\r\n",
        "\r\n",
        "    sent_id, mask, labels = batch\r\n",
        "\r\n",
        "    # deactivate autograd\r\n",
        "    with torch.no_grad():\r\n",
        "      \r\n",
        "      # model predictions\r\n",
        "      preds = model(sent_id, mask)\r\n",
        "\r\n",
        "      # compute the validation loss between actual and predicted values\r\n",
        "      loss = cross_entropy(preds,labels)\r\n",
        "\r\n",
        "      total_loss = total_loss + loss.item()\r\n",
        "\r\n",
        "      preds = preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "      total_preds.append(preds)\r\n",
        "\r\n",
        "  # compute the validation loss of the epoch\r\n",
        "  avg_loss = total_loss / len(val_dataloader) \r\n",
        "\r\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iJHcSeA3pik",
        "outputId": "c8c7d5d4-68e2-4e2b-d4fc-fb2726ee6e39"
      },
      "source": [
        "# set initial loss to infinite\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "# empty lists to store training and validation loss of each epoch\r\n",
        "train_losses=[]\r\n",
        "valid_losses=[]\r\n",
        "\r\n",
        "#for each epoch\r\n",
        "for epoch in range(epochs):\r\n",
        "     \r\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n",
        "    \r\n",
        "    #train model\r\n",
        "    train_loss, _ = train()\r\n",
        "    \r\n",
        "    #evaluate model\r\n",
        "    valid_loss, _ = evaluate()\r\n",
        "    \r\n",
        "    #save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    # append training and validation loss\r\n",
        "    train_losses.append(train_loss)\r\n",
        "    valid_losses.append(valid_loss)\r\n",
        "    \r\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.489\n",
            "Validation Loss: 0.384\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.374\n",
            "Validation Loss: 0.348\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.355\n",
            "Validation Loss: 0.341\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.347\n",
            "Validation Loss: 0.337\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.341\n",
            "Validation Loss: 0.332\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.336\n",
            "Validation Loss: 0.333\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.333\n",
            "Validation Loss: 0.331\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.333\n",
            "Validation Loss: 0.326\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.328\n",
            "Validation Loss: 0.323\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch   500  of  4,375.\n",
            "  Batch 1,000  of  4,375.\n",
            "  Batch 1,500  of  4,375.\n",
            "  Batch 2,000  of  4,375.\n",
            "  Batch 2,500  of  4,375.\n",
            "  Batch 3,000  of  4,375.\n",
            "  Batch 3,500  of  4,375.\n",
            "  Batch 4,000  of  4,375.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    938.\n",
            "  Batch   100  of    938.\n",
            "  Batch   150  of    938.\n",
            "  Batch   200  of    938.\n",
            "  Batch   250  of    938.\n",
            "  Batch   300  of    938.\n",
            "  Batch   350  of    938.\n",
            "  Batch   400  of    938.\n",
            "  Batch   450  of    938.\n",
            "  Batch   500  of    938.\n",
            "  Batch   550  of    938.\n",
            "  Batch   600  of    938.\n",
            "  Batch   650  of    938.\n",
            "  Batch   700  of    938.\n",
            "  Batch   750  of    938.\n",
            "  Batch   800  of    938.\n",
            "  Batch   850  of    938.\n",
            "  Batch   900  of    938.\n",
            "\n",
            "Training Loss: 0.326\n",
            "Validation Loss: 0.319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryVfiRmS3xFS",
        "outputId": "c7f1d4e2-636d-4e72-91fd-0a0fd76052f0"
      },
      "source": [
        "#load weights of best model\r\n",
        "path = 'saved_weights.pt'\r\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u3sx1kk3GpP8",
        "outputId": "d163c56b-3ca3-44e3-b83f-abedc6957475"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPVlA5MNJifG"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLuY6v4yKQl2",
        "outputId": "0b0fcae2-d623-447c-a440-340c76af1faf"
      },
      "source": [
        "test_seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101, 17170, 11344,  ...,     0,     0,     0],\n",
              "        [  101, 17284,  6827,  ...,     0,     0,     0],\n",
              "        [  101, 12223,  3170,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101, 20222,  2308,  ...,  2620,  2549,   102],\n",
              "        [  101, 16619,  3256,  ...,  1022, 13109,   102],\n",
              "        [  101,  2452, 17955,  ..., 19968,  5835,   102]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqXac3SxFcw4"
      },
      "source": [
        "# get predictions for test data\r\n",
        "with torch.no_grad():\r\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\r\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRQLUsldhElA",
        "outputId": "c915f86f-8019-4b11-c08a-5d1c7d4bb6cd"
      },
      "source": [
        "preds = np.argmax(preds, axis = 1)\r\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92     10060\n",
            "           1       0.83      0.83      0.83      4940\n",
            "\n",
            "    accuracy                           0.89     15000\n",
            "   macro avg       0.88      0.88      0.88     15000\n",
            "weighted avg       0.89      0.89      0.89     15000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}